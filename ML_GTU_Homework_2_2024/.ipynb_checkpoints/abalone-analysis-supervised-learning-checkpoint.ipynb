{"cells":[{"metadata":{"_uuid":"987bc944c51469bb4fa22b6f3aec49e13887b434"},"cell_type":"markdown","source":"#                    Predicting The Age of Abalone üê≥üê†\n\n![coffee](https://images.unsplash.com/photo-1495004984586-0dc339ad4b2c?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3a6bedd3ca3b1ced9521b663a8f4ccfd&auto=format&fit=crop&w=667&q=80)\n\n*courtesy of Sasha ‚Ä¢ Stories from Unsplash*"},{"metadata":{"_uuid":"b8681c52a8460d812798f5039935bd518554636b"},"cell_type":"markdown","source":"## Overview\n\nThe main goal of this notebook is to explore three different Supervised Learning Algorithms: Decision Trees, Random Forest and XGBoost.  I will explore handling missing values, Creating Pipelines, Partial Dependance Plots and One-hot Encoding. \n\nThe focus is on predicting the age of Abalone. These are sea snails that are quite in dangered in South Africa at least, but can be found in Australia, Great Britian and New Zealand. \n\n![abalone shell](https://cdn.shopify.com/s/files/1/2086/1263/products/1d89434927bffb6fd1786c19c2d921fb_2000x.jpg?v=1522240385)\n\n*An Abalone shell*\n\nWe will use various measurements to predict the number of rings the abalone have, which determines it's age once 1.5 is added to the total rings.  "},{"metadata":{"_uuid":"974b86a78a844906691475a42fa1e55dba2dfa61"},"cell_type":"markdown","source":"## Table of Contents\n\n* Import packages & Data\n* Exploratory Data Analysis\n* Data Cleaning\n* Feature Engineering \n* Train Models\n* Evaluation of Models\n* Conclusion\n* Resources Used"},{"metadata":{"_uuid":"2a971d27d25bfc1f9fab5ec7cf7cdeb4afeb2976"},"cell_type":"markdown","source":"# Import packages & Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Standard libs\nimport numpy as np \nimport pandas as pd \n\n#Data Visualisation libs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\n\n#Feature engineering, metrics and modeling libs\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing.imputation import Imputer\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\n#Detect Missing values\nimport missingno as msno\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd900ad5c3a993a334e032016c297a1a1f175888"},"cell_type":"markdown","source":"Lets read the data into a Pandas dataframe:\n\n"},{"metadata":{"trusted":true,"_uuid":"bfd0b90c99e84be7c53ab6bb04a582f649237fa4"},"cell_type":"code","source":"abalone = pd.read_csv('../input/abalone.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23b85dc1f6472995fdb00feb01571ccd296a59bc"},"cell_type":"code","source":"abalone.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a1bece12486e08091d315286699f2da2e74e7a9"},"cell_type":"code","source":"abalone.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcdcd61a213c485b265181fbaeac2c3dd5ccdb40"},"cell_type":"code","source":"abalone.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a66298f805b76383cf125d2c92ecf551586982fb"},"cell_type":"markdown","source":"Before I give my inital reaction to the data please note the following units of measurement for the various variables:\n\n- `Length` - mm\n- `Diameter` - mm\n- `Height` - mm\n- `Whole weight` - grams\n- `Shucked weight` - grams\t\n- `Viscera weight` -grams\t\n- `Shell weight` -grams\n\nFor the rings you need add 1.5 to get the age in years."},{"metadata":{"_uuid":"010e280b6c9fdb356a95c031231c96182c949b9b"},"cell_type":"markdown","source":"We can see that the mean length from the observations in the dataset is 0.52 mm, while the longest is 0.82 mm and the shortest is 0.075 which is tiny! With the height we get a mininum of 0.0000 which is suspect because that must be a bogus observation. Shell weight seems to not be more than 1 gram, while the oldest Abalone is 30.5 years old and the youngest 2.5 years. The heaviest abalone is 2.83 grams and the lightest 0.001. "},{"metadata":{"trusted":true,"_uuid":"0df53ebb03b559fea43f95fb888677a7415dafce"},"cell_type":"code","source":"abalone.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e072e1e6e6f4c0c5b6663fcb398b00501cabb883"},"cell_type":"markdown","source":"We only have one categorical column while the other eight are numerical. Lets go ahead and see if there are any null values:"},{"metadata":{"trusted":true,"_uuid":"d16ee737eb64abba411e0c0a57d0d9438b654103"},"cell_type":"code","source":"len(abalone.isnull())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aedef4630148aa0a3f14481c335fd3d3edd88e82"},"cell_type":"markdown","source":"The `isnull()` method returns a boolean of true or false for the detection of null values.  In our case it has returned `false` for 4177 entries in the dataset, this matches the same amount of entries in the entire dataset. So we can be safe knowing we have no null values."},{"metadata":{"trusted":true,"_uuid":"311bead482dbb585b4a7cda48f081d238fd95049"},"cell_type":"markdown","source":"# Exploratory Data Analysis\n"},{"metadata":{"_uuid":"4e48189658888c2208b2e6b5a2ff0df85618662a"},"cell_type":"markdown","source":"Lets go ahead and see the skewness of the target variable  which is `Rings`"},{"metadata":{"_uuid":"2097d763e34f9292f0c013bf20631c5efd8d913d","trusted":true},"cell_type":"code","source":"rings = abalone['Rings']\nplt.figure(1);plt.title('Normal')\nsns.distplot(rings,kde=False,fit=st.norm)\nplt.figure(2);plt.title('Johnson SU')\nsns.distplot(rings,kde=False,fit=st.johnsonsu)\nplt.figure(3);plt.title('Log Normal')\nsns.distplot(rings,kde=False,fit=st.lognorm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc32e1b1b70a4a22cd9a5c90d80879f4f040f98f"},"cell_type":"markdown","source":"None of theses distributions fit our data perfectly. So we need to find another one to match our data.\n\n"},{"metadata":{"trusted":true,"_uuid":"2f311e1cbec68ed27f53114dce0facfa72305459"},"cell_type":"code","source":"plt.hist(rings,color='green')\nplt.title('Histogram of rings')\nplt.xlabel('Number of Rings')\nplt.ylabel('count')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b782dcd90bb70fdf2c3d7c8b799122e25f817461"},"cell_type":"markdown","source":"I would like observe which variables are strongly correlated to one another:\n\n"},{"metadata":{"trusted":true,"_uuid":"b14b4e6d532afd093afdfecc4aff467a02d28388"},"cell_type":"code","source":"numeric_features = abalone.select_dtypes(include=[np.number])\ncorrelation = numeric_features.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8151ed70a2b8989d8013350c0edc826a24b4e32"},"cell_type":"code","source":"print(correlation['Rings'].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d6f748986b0aa9a16814e3561f9b46239c1706b"},"cell_type":"code","source":"plt.title('Correlation of numeric features', y=1,size=16)\nsns.heatmap(correlation,square=True,vmax=0.8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be2ce38a3bf95367dd3d6e8cada325865eb04a43"},"cell_type":"markdown","source":"It appears that Shell weight is the only feature that has a correlation above 0.50. All the other features linger around 0.42 - 0.51, I feel this will be a problem when making predictions. "},{"metadata":{"trusted":true,"_uuid":"d2a1f9832beca34dac9368a2db72761ade3445c2"},"cell_type":"code","source":"cols = ['Rings','Shell weight','Diameter','Height','Length']\nsns.pairplot(abalone[cols],size=2,kind='scatter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4432eed19e0a63054fea3466283e624e8b0d0a4f"},"cell_type":"markdown","source":"These scatter plots give us a different overview of the correlated features. This is what I've picked up:\n\n* `Height` and `Length` have an interesting relationship in that Abalone's with a height of 0.0 mm - 0.5 mm have varing degrees of length. `Length` and `Diameter` seem to have a nice linear relationship with few outliers. \n* It appears that the the number of `Rings` of an Abalone are all concentrated around a `Height` below 0.5 mm.\n* It seems that many Abalone have `Rings` between 5-20 that have`Shell weight`  distributed above 0.00 grams to 0.75 grams. Also the larger in `Diameter` the Abalone is the more `Shell weight` it has too. "},{"metadata":{"trusted":true,"_uuid":"53fd56a5bbe23c53eab471802ba3e59b29499be6"},"cell_type":"code","source":"sns.countplot(abalone['Sex'], palette=\"Set3\")\nplt.title('Count of the Gender of Abalone')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79c8442cb431f03eab49d9c0a4bb89a6129f0277"},"cell_type":"markdown","source":"Males are the most frequent `Sex` of Abalone in the dataset, while Infants just edge out Females. "},{"metadata":{"trusted":true,"_uuid":"d54592a577b9c15b96f494c29a2906b9a15e73d8"},"cell_type":"code","source":"sns.countplot(abalone['Rings'])\nplt.title('Distribution of Rings')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a51e9c173ac76f4307ef9602f90458a8cd1b519d"},"cell_type":"markdown","source":"It appears that Abalone with `Rings` between 8-10 have the most observations."},{"metadata":{"trusted":true,"_uuid":"efcd49c61c64004803c4f41ff0f3aab3ea31f3cd"},"cell_type":"code","source":"p = sns.FacetGrid(abalone, col=\"Sex\", hue=\"Sex\")\np=p.map(plt.scatter,\"Rings\", \"Shell weight\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"026f0d3f64010fcc76f4891f7a3ee9c14e28fdc4"},"cell_type":"markdown","source":"With this scatter plot I took the `Shell weight` because it has the highest correlation with `Rings` and tried to see how they compare against the different sexes. \n\nWhat first catches the eye is that we can finally see a cut off age for `Rings` in infants which in the region of 20. While Male and Female have a similar distribution, with the Males having more visible outliers. "},{"metadata":{"trusted":true,"_uuid":"20e8d3d04151efa87edae5bfb66fe31755cff0dd"},"cell_type":"code","source":"x = sns.FacetGrid(abalone,col=\"Sex\",hue=\"Sex\")\nx.map(plt.scatter, \"Rings\", \"Diameter\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96aad040df9132c35dff96c6f388b1dc4a178c81"},"cell_type":"markdown","source":"In this Case it is the second highest correlated variable `Diameter`. Females only have observations starting from 0.2 mm in diameter and with clear outliers. Otherwise Males and Infants have rough a similar spread of the data."},{"metadata":{"trusted":true,"_uuid":"89302e00fb661ca1ef2f5d6b9ff38cb75c08fe42"},"cell_type":"code","source":"f = (abalone.loc[abalone['Sex'].isin(['M','F'])]\n      .loc[:,['Shell weight','Rings','Sex']])\n\nf = f[f[\"Rings\"] >= 8]\nf = f[f[\"Rings\"] < 23]\nsns.boxplot(x=\"Rings\",y=\"Shell weight\", hue='Sex',data=f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c17d2f635d0c82bfa2dd8b49407a1cade4354a1"},"cell_type":"markdown","source":"This box plot allows us to clearly visualize the presence of outliers, meaning we will need to manage them before training on the data."},{"metadata":{"trusted":true,"_uuid":"5602d9310df76e6efb03f4f5924d9f2cdae06fa6"},"cell_type":"code","source":"w = (abalone.loc[abalone['Sex'].isin(['I'])]\n    .loc[:,['Shell weight','Rings','Sex']])\nw = w[w[\"Rings\"] >= 8]\nw = w[w[\"Rings\"] < 23]\nsns.boxplot(x=\"Rings\",y=\"Shell weight\", hue='Sex',data=w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc87f291770a2201d86012cc5d2d2dcbd345d803"},"cell_type":"markdown","source":"As there is only one signifincant outlier while it is clear that infants have `Rings` in the range of 8 -21."},{"metadata":{"trusted":true,"_uuid":"5c0ac28217cab541af1d478397aaa6f07c0a15e1"},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(14,10))\n\nShellWeight_plot = pd.concat([abalone['Rings'],abalone['Shell weight']],axis=1)\nsns.regplot(x='Rings',y='Shell weight',data=ShellWeight_plot,scatter=True,fit_reg=True,ax=ax1)\n\nDiameter_plot = pd.concat([abalone['Rings'],abalone['Diameter']],axis=1)\nsns.regplot(x='Rings',y='Diameter',data=Diameter_plot,scatter=True,fit_reg=True,ax=ax2)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6709e345e04fa1fd08795f19ae5025b0b739c032"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"01132beb959783e7283182ba4b3536f9864a62db"},"cell_type":"markdown","source":"#  Data Cleaning\n\nFrom the EDA it is clear that we will need to do something to remove outliers. To achieve this we can use the Scipy stats module to convert the observations into Zscores. Zscores give us a distribution that illustrate how many standard deviations away from the mean the data is. \n\n"},{"metadata":{"trusted":true,"_uuid":"953174de9582ddf97c4fd6ff2f448c0a2d43c8f7"},"cell_type":"code","source":"from scipy import stats\nz= np.abs(stats.zscore(abalone.select_dtypes(include=[np.number])))\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ed62f8f3fbc1829756a0a77e0881abe90a1f389"},"cell_type":"code","source":"abalone_o = abalone[(z < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c7f20595cf6ebdea8466cdf1d3a97472a935d76"},"cell_type":"markdown","source":"What has happened above is that we are removing all the data points that are below three standard deviations away from the mean."},{"metadata":{"trusted":true,"_uuid":"621e237127ab37335f423203ccf2c0ed7be8a46b"},"cell_type":"code","source":"print(\"Shape of Abalones with outliers: \"+ str(abalone.shape) , \n      \"Shape of Abalones without outliers: \" + str(abalone_o.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e73b640b3184f448c740f76811dd127b194cb9dc"},"cell_type":"markdown","source":"#  Feature Engineering \n"},{"metadata":{"_uuid":"3e734008ec91788f28f4ab680d86c0945874d805"},"cell_type":"markdown","source":"###  One Hot encoding\n\nWe will use a technique called one-hot encoding to create binary columns that will show the presence of each possible value from the original dataset. This is because the model will not know how to handle 'M', 'F' or 'I' when it processes it.\n\nHowever first I will need to determine the cardinality of columns:\n\n"},{"metadata":{"trusted":true,"_uuid":"5fd141807f5dfdefb978823c9404c3d832792486"},"cell_type":"code","source":"low_cardinality_cols = [cname for cname in abalone_o.columns if\n                        abalone_o[cname].nunique() < 10 and \n                       abalone_o[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in abalone_o.columns if\n                                 abalone_o[cname].dtype in ['int64','float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\nabalone_predictors = abalone_o[my_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd8424230b4c0b826abd74d33aaf6195d6f0b352"},"cell_type":"code","source":"abalone_predictors.dtypes.sample(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"280007bb77567f7539148923a0e8ac49267e3046"},"cell_type":"markdown","source":"Now we can encode them with the `get_dummies()` method available to a Pandas Dataframe:"},{"metadata":{"trusted":true,"_uuid":"ae2bbb0afa0a1e1e5948027624acee0991ec3ffe"},"cell_type":"code","source":"abalone_encoded_predictors = pd.get_dummies(abalone_predictors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80e650e1a65e23cd86548cd8809d8b2652dae4bc"},"cell_type":"code","source":"abalone_encoded_predictors.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bad486544e19017cb45c761456f91c9f992d1c9"},"cell_type":"code","source":"abalone_encoded_predictors.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f25fd23bff300d3e062112f81fdf58374d57325c"},"cell_type":"markdown","source":"Our abalone_encoded_predictors dataframe now has three new columns `Sex_F`,  `Sex_I` and  `Sex_M`."},{"metadata":{"_uuid":"e8b9222f93addc950c4ff3200b046f3159be73b4"},"cell_type":"markdown","source":"#  Train Models\n\nAs mentioned earlier we will be exploring a few models: \n\n* Decision Trees, \n* Random Forest \n* XGBoost\n\nEach one will be trained with a SkLearn Pipeline and with Cross Validation to compare which method gives us greater accuracy. Key thing to note is that we will be performing Regression with these models, Regression is the study of a depanant variable on one or more explanatory variables. \n"},{"metadata":{"_uuid":"af0b14fe6d0131637f2ba72c4b972a9275dbe30f"},"cell_type":"markdown","source":"### Decision Trees\n\nDecision Trees allow us to use certain criteria which would be the models features to predict/classifiy a target. The data is split into smaller sets based on a feature then it will run a prediction on that feature. \n\nYou can read more about them [here](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) and [here](https://medium.com/@SeattleDataGuy/what-is-a-decision-tree-algorithm-4531749d2a17).\n\nSkLearn offers two types of Decision Trees: a classifer and regressor. Because we are not trying to determine an event we will use the regressor. "},{"metadata":{"_uuid":"f2ec47c0fcec2a6f96cb93ee70384b6698d54905"},"cell_type":"markdown","source":"#### Cross Validation"},{"metadata":{"trusted":true,"_uuid":"a7c6741362572b7cf1d25759cfad5c152a2da00d"},"cell_type":"code","source":"cross_cols = ['Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Sex_F','Sex_I','Sex_M']\nX = abalone_encoded_predictors[cross_cols]\ny = abalone_encoded_predictors.Rings\n\ndecision_pipeline = make_pipeline(DecisionTreeRegressor())\ndecision_scores = cross_val_score(decision_pipeline, X,y,scoring='neg_mean_absolute_error')\n\nprint('MAE %2f' %(-1 * decision_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42026533f7d352cb53e7f24e9bfc3be2293faa18"},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true,"_uuid":"03a47250f44bdc7abf16a96df46557cb780714b6"},"cell_type":"code","source":"dt_train_X,dt_test_X,dt_train_y,dt_test_y = train_test_split(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a85a7da4784eabead90cb84136421a0c5e42d0"},"cell_type":"markdown","source":"With Decision Trees you are able to run a variation of them to determine how many leaf nodes produces the best set of results. Below we create the pipeline in the `get_mae()` function to return the mean absolute error."},{"metadata":{"trusted":true,"_uuid":"4beafc66612fd073174de6be2d595fd193b2efc9"},"cell_type":"code","source":"def get_mae(max_leaf_nodes,dt_train_X,dt_test_X,dt_train_y,dt_test_y ):\n    model_pipeline = make_pipeline(DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0))\n    model_pipeline.fit(dt_train_X,dt_train_y)\n    preds_val = model_pipeline.predict(dt_test_X)\n    mae = mean_absolute_error(dt_test_y,preds_val)\n    return(mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a336994b63c24ef02bd9db6e43f10e65bb208382"},"cell_type":"markdown","source":"Now we can loop over the different leaf nodes to determine which one will give us the lowest MAE."},{"metadata":{"trusted":true,"_uuid":"1d395ff21cf68d380219a3ef544180ef87bbaa47"},"cell_type":"code","source":"for max_leaf_nodes in [5,50,500,5000]:\n    my_mae = get_mae(max_leaf_nodes,dt_train_X,dt_test_X,dt_train_y,dt_test_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d\" %(max_leaf_nodes,my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65633b7ef7eb64503e559304086a2526ec8e3258"},"cell_type":"markdown","source":"From this it is clear that any number of leaf nodes will result in the same value for the MAE. I suspect due to the size of the dataset this might be the cause. "},{"metadata":{"trusted":true,"_uuid":"82966a2c7b0b4b00ba15cc5cf440263e7efd6f4b"},"cell_type":"code","source":"decision_split_pipeline = make_pipeline(DecisionTreeRegressor(max_leaf_nodes=5))\ndecision_split_pipeline.fit(dt_train_X,dt_train_y)\ndecision_tree_prediction = decision_split_pipeline.predict(dt_test_X)\nprint(\"MAE: \" + str(mean_absolute_error(decision_tree_prediction,dt_test_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"091b9cd5b01096aed19fbb183730e9cdf0428a8c"},"cell_type":"code","source":"acc_decision = decision_split_pipeline.score(dt_test_X,dt_test_y)\nprint(\"Acc:\", acc_decision )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc27c88d896ea73fcadf4fe1534c906a16da98e4"},"cell_type":"markdown","source":"With an accuracy score of 44% our model is performing very poorly.  The MAE for Cross-val is :`1.959743` while for Train-test it is: `1.597016`. In this case the using train-test split for the Decision Tree is best."},{"metadata":{"trusted":true,"_uuid":"7cbf4c1f142f6431ad176f322c069d35c7b2c233"},"cell_type":"code","source":"plt.scatter(dt_test_y,decision_tree_prediction,color='green')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Decision Tree: Actuals vs Predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2675a9076db74d31aa90071486fa3e1d58c4c18a"},"cell_type":"markdown","source":"### Random Forests\n\nRandom Forests randomly chooses observations and features to generate many decision trees to average the result. While a Decision tree generates rules based on the features in the data. \n\nThe main issue with Random Forests is that they are slow to gather real-time predictions on, but are extremly fast to train.  However, RF prevents overfitting by generating trees on random subsets. "},{"metadata":{"trusted":true,"_uuid":"dfcc1f1ead25d81bf21e4e5915aba3599905baac"},"cell_type":"markdown","source":"#### Cross Validation"},{"metadata":{"trusted":true,"_uuid":"891a2ac0b6913f8a2e08537ea37ea750fdcc1491"},"cell_type":"code","source":"forest_pipeline = make_pipeline(RandomForestRegressor(random_state=1))\nforest_scores = cross_val_score(forest_pipeline, X,y,scoring=\"neg_mean_absolute_error\")\nprint('MAE %2f' %(-1 * forest_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26d5cd7ca237b19dd96f6780fc172841d47b23e8"},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true,"_uuid":"bb85e3872debe65a5afe0210a8da783959a6e745"},"cell_type":"code","source":"f_train_X,f_test_X,f_train_y,f_test_y = train_test_split(X,y)\nforest_split_pipeline = make_pipeline(RandomForestRegressor(random_state=1))\nforest_split_pipeline.fit(f_train_X,f_train_y)\nforest_predictions = forest_split_pipeline.predict(f_test_X)\nprint(\"Accuracy:\",forest_split_pipeline.score(f_test_X,f_test_y))\nprint(\"MAE:\",str(mean_absolute_error(forest_predictions,f_test_y)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1b0a4c1193c54154796902118397cd774b5c9c5"},"cell_type":"markdown","source":"Yet again the train-test split has out performed the cross-validation, with a MAE of `1.511744` vs a MAE of `1.521251`. The Random Forest model also has an accuracy of 48%. These are marginal improvements!"},{"metadata":{"trusted":true,"_uuid":"46d63b7bf23f13a0c822fc16b2872b377fe77704"},"cell_type":"code","source":"plt.scatter(f_test_y,forest_predictions,color='red')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Actuals vs Predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d61ff58fbf2dd839682c05d95c0ac26da7df875"},"cell_type":"markdown","source":"This plots clearly shows where the model is lacking, if you look at the actuals the model does poorly in some cases with its predictions. "},{"metadata":{"_uuid":"83b5d535367d5585ce5b605cd875325e99b5ea1b"},"cell_type":"markdown","source":"### XGBoost\n\nTo understand eXtreme Gradient Boosting, we need to understand the logic behind boosting algorithims. They build various weak models to make conclusions about the various importance of features to reduce them. Ultimately, XGBoost is an implementaion of Gradient Boosted Decision Trees. There are three different forms of gradient boosting : \n* Gradient Boosting algorithim\n* Stochastic Gradient Boosting\n* Regularized Gradient Boosting \n\n\nXGBoost gives us great speed in running our models and good model performance. \n"},{"metadata":{"_uuid":"9d88bd5107ff3c0df5fcd5949ca749f18a7a0c62"},"cell_type":"markdown","source":"#### Cross Validation"},{"metadata":{"trusted":true,"_uuid":"8387b85716726e02dece25ea8a3f103f75cd0efb"},"cell_type":"code","source":"xgb_pipeline = make_pipeline(XGBRegressor())\nxgb_scores = cross_val_score(xgb_pipeline,X.as_matrix(),y.as_matrix(),scoring=\"neg_mean_absolute_error\")\nprint(\"MAE %2f\" %(-1 * xgb_scores.mean()) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ebb709b099257ea88044aa4e6fcb67c62d4eb67"},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true,"_uuid":"181c3d8a87d312b2ee38a8153cf7e07713adb398"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split(X.as_matrix(),y.as_matrix(),test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1aa22f9b7b090e03b1145e47f5cc47c622ef425"},"cell_type":"markdown","source":"I was unable to apply the pipeline on this model as it keep printing out the following error: ` Last step of Pipeline should implement fit.` so I opted to just fit it without a pipeline."},{"metadata":{"trusted":true,"_uuid":"53be796e241fa0c5afb77b10c51ef276db16f024"},"cell_type":"code","source":"xgb_model = XGBRegressor()\nxgb_model.fit(train_X,train_y,verbose=False)\nxgb_preds = xgb_model.predict(test_X)\nprint(\"MAE: \" + str(mean_absolute_error(xgb_preds,test_y)))\nprint(\"Accuracy:\",xgb_model.score(test_X,test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5736402acb0043d389aca0509064f23fc04ed675"},"cell_type":"markdown","source":"Below I opted to train the model again by explictly stating the number of estimators and learning rate."},{"metadata":{"trusted":true,"_uuid":"abedfbb0ffd5170ab092eb4f887424a7db72e266"},"cell_type":"code","source":"xgb_model_II = XGBRegressor(n_estimators=1000,learning_rat=0.05)\nxgb_model_II.fit(train_X,train_y,early_stopping_rounds=5,\n             eval_set=[(test_X,test_y)],verbose=False)\nxgb_preds = xgb_model_II.predict(test_X)\nprint(\"MAE: \" + str(mean_absolute_error(xgb_preds,test_y)))\nprint(\"Accuracy:\",xgb_model_II.score(test_X,test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f160a0f61dbaad15774d7b6e4e569493218b860"},"cell_type":"markdown","source":"We can gauge that the improvement is slightly marginal.  The difference between the XGBoost and Forests is also slightly marginal. "},{"metadata":{"trusted":true,"_uuid":"cfdc0af91a35d329310c4235056734eea5c2f23f"},"cell_type":"code","source":"plt.scatter(test_y,xgb_preds,color='blue')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Actuals vs Predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76b5cf11fa61c7417c206357e509c354b6cd8df5"},"cell_type":"markdown","source":"## Partial Dependence Plots "},{"metadata":{"_uuid":"80d2d36560b3214ec3e440dc8742c45f71f094d5"},"cell_type":"markdown","source":"Partial Dependence Plots display the effect (marginally) of a feature on the predicted outcome of a model that has just been fitted."},{"metadata":{"trusted":true,"_uuid":"7546c95c662fbe9cd014b2ace3a40c8733ad68a4"},"cell_type":"code","source":"cols = ['Length','Diameter','Height']\nab_par_model = GradientBoostingRegressor()\nab_par_model.fit(X,y)\nmy_plots = plot_partial_dependence(ab_par_model,\n                                  features=[0,2],\n                                  X=X,\n                                  feature_names=cols,\n                                   grid_resolution=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d508288eec7dcdcc868843f2ea52380af94f560"},"cell_type":"markdown","source":"In our case it seems that Height has more of an effect of determining the number of rings on Abalone than length. "},{"metadata":{"_uuid":"406ad995f3f187489abd0a5155515ae9252a4814"},"cell_type":"markdown","source":"# Evaluation of Models\n\n| Models        | MAE: Cross-val | MAE:Train-test  | Accuracy| Rank|\n| ------------- |:-------------:| -----:| ----:|-----:|\n| Decision Tree     | 1.959743 |1.597018 |0.44 |3rd|\n| Random Forests   | 1.511744   | 1.521251 |0.48|2nd|\n| XGBoost |1.424527    |    1.402607 |0.56|1st|\n\n**These results are from v20 of the notebook, all the other MAE & accuracy scores are from v20 of the notebook.**"},{"metadata":{"_uuid":"ea2045540657146ace419f4d4dfd4dcda91f9e23"},"cell_type":"markdown","source":"# Conclusion\n\nThe XGBoost model is clearly the superior model however, it's accuracy is only just above 50% which is pretty average at best. Also, it seems that the train-test split might be the best path  to train the model. However, with the Decision Tree the cross-validation has the highest MAE. So overall using that techinque does not improve our model has I would've hoped at first.\n\nThe next steps would be to do more feature engineering and some hyperparameter tuning to improve the model. With respects to improvements, having a pipeline that runs all three models at the same time and prints the results together could be positive step. \n\n\nFinally to recap, we have covered:\n\n* How to implement Sklearn pipelines.\n* How to create Decision Tree, Random Forests and XGBoost regression models.\n* How to detect outliers and missing values.\n* How to use cross validation.\n* How to implement One-Hot Encoding.\n* How to create Partial Dependance Plots."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Resources Used Manufacture This Notebook:\n\n* [Detecting_Removing_Outliers]( https://github.com/SharmaNatasha/Machine-Learning-using-Python/blob/master/Simple%20projects/Detecting_Removing_Outliers.ipynb)\n\n* [Data Modeling](https://github.com/AlexIoannides/ml-project-workflow/blob/master/Data%20Modelling.ipynb)\n\n* [Random Forest Algorithm]( https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd)\n\n* [random-forest-simple-explanation](https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d)\n* [random-forests-classifier-python](https://www.datacamp.com/community/tutorials/random-forests-classifier-python)\n* [gentle-introduction-xgboost]( https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n* [Partial Dependance Plots]( https://christophm.github.io/interpretable-ml-book/pdp.html)\n* [A Simple Tutorial on Exploratory Data Analysis](https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis)\n* [Churn Prediction of Telco Customers](https://www.kaggle.com/meldadede/churn-prediction-of-telco-customers)\n* [Data Camp](https://github.com/AmoDinho/datacamp-python-data-science-track)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}